{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b6f830-ac6b-4291-a9f1-3a8c780cdbc2",
   "metadata": {},
   "source": [
    "# OPPT Tutorial (USING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87397ff-5ae2-446c-90c1-6a8a877cc68d",
   "metadata": {},
   "source": [
    "### OPPT Examples\n",
    "Tiger Problem Example will be uploaded to a public repository.\n",
    "I will send the link to the public repository once uploaded.\n",
    "Feel free to suggest any changes.\n",
    "\n",
    "\n",
    "### OPPT Documents\n",
    "Compile an OPPT FAQ document with common difficulties from install, running an example provided, and implementing own problem.\n",
    "This would help improve OPPT and documentation for future updates.\n",
    "\n",
    "\n",
    "### Collaboration\n",
    "Work together in an oppt slack or teams to help each other on common problems faced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7a638-f458-425b-aec4-56be66b407f9",
   "metadata": {},
   "source": [
    "# OPPT handy hacks for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea591609-0476-4232-ba75-2749213f01aa",
   "metadata": {},
   "source": [
    "#### \"getchar()\" after each execution step in ProblemEnvironment.hpp\n",
    "We can place a \"getchar()\" statement after each execution step to track what is happening during an experiment run: \n",
    "\n",
    "\n",
    "\"\"\" Example of run function in ProblemEnvironment.hpp \"\"\"\n",
    " SimulationResult run(const unsigned int& run, std::ofstream & os, int argc, char const * argv[]) {\n",
    "        \n",
    "        ... Some setup code happening here ...\n",
    "        \n",
    "        # Sample an initial state\n",
    "        RobotStateSharedPtr currentState =\n",
    "            robotExecutionEnvironment_->sampleInitialState();\n",
    "       \n",
    "        ... Some more setup code here ....\n",
    "        \n",
    "        FloatType t0 = oppt::clock_ms();\n",
    "        while (true) {\n",
    "            cout << \"\\nt = \" << currentStep << endl;\n",
    "            stepResult = StepResult(currentStep, robotExecutionEnvironment_.get());\n",
    "            stepResult.currentState = currentState;\n",
    "            stepResult.discountFactor = currentDiscount;\n",
    "            if (currentStep >= problemEnvironmentOptions_->nSimulationSteps) {\n",
    "                // We're out of steps\n",
    "                PRINT(\"Out of steps. Ending simulation run\");\n",
    "                finalState = currentState;\n",
    "                break;\n",
    "            }\n",
    "\n",
    "            if (problemEnvironmentOptions_->hasVerboseOutput)\n",
    "                cout << \"Improving policy...\\n\";\n",
    "\n",
    "            // Improve the current policy\n",
    "            FloatType planningTimer = oppt::clock_ms();\n",
    "            if (!solver_->improvePolicy(problemEnvironmentOptions_->stepTimeout)) {\n",
    "                finalState = currentState;\n",
    "                totalPlanningTime += (oppt::clock_ms() - planningTimer) / 1000.0;\n",
    "                LOGGING(\"Couldn't improve policy. Ending simulation run\");\n",
    "                break;\n",
    "            }\n",
    "\n",
    "            totalPlanningTime += (oppt::clock_ms() - planningTimer) / 1000.0;\n",
    "\n",
    "            // Get nextAction\n",
    "            ActionSharedPtr action = solver_->getNextAction();\n",
    "            if (!action) {\n",
    "                // No action to execute\n",
    "                finalState = currentState;\n",
    "                LOGGING(\"No action for the next step. Ending simulation run\");\n",
    "                break;\n",
    "            }\n",
    "\n",
    "            cout << \"action: \" <<\n",
    "                 *(robotPlanningEnvironment_->getRobot()->getActionSpace()->denormalizeAction(action))\n",
    "                 << endl;\n",
    "            stepResult.action = action;\n",
    "\n",
    "            // Execute action\n",
    "            PropagationRequestSharedPtr propagationRequest(new PropagationRequest());\n",
    "            propagationRequest->currentState = currentState;\n",
    "            propagationRequest->action = action.get();\n",
    "            propagationRequest->allowCollisions = problemEnvironmentOptions_->allowCollisions;\n",
    "\n",
    "            oppt::PropagationResultSharedPtr propagationResult =\n",
    "                robotExecutionEnvironment_->getRobot()->propagateState(propagationRequest);\n",
    "            if (propagationResult->nextState) {\n",
    "                cout << \"Next state: \" <<\n",
    "                     *(robotPlanningEnvironment_->getRobot()->getStateSpace()->denormalizeState(propagationResult->nextState).get())\n",
    "                     << endl;\n",
    "            } else {\n",
    "                cout << \"Next state: NULL\" << endl;\n",
    "            }\n",
    "\n",
    "            // Get an observation\n",
    "            ObservationRequestSharedPtr observationRequest(new ObservationRequest());\n",
    "            observationRequest->currentState = propagationResult->nextState;\n",
    "            observationRequest->action = action.get();\n",
    "            ObservationResultSharedPtr observationResult =\n",
    "                robotExecutionEnvironment_->getRobot()->makeObservationReport(observationRequest);\n",
    "\n",
    "            stepResult.observation = observationResult->observation;\n",
    "            auto observationSpace = robotPlanningEnvironment_->getRobot()->getObservationSpace();\n",
    "            cout << \"Observation: \" <<\n",
    "                 *(observationSpace->denormalizeObservation(stepResult.observation).get()) <<\n",
    "                 endl;\n",
    "\n",
    "            // Get a reward\n",
    "            FloatType reward = robotExecutionEnvironment_->getReward(propagationResult);\n",
    "\n",
    "            totalDiscountedReward += currentDiscount * reward;\n",
    "            stepResult.reward = reward;\n",
    "            cout << \"Immediate reward: \" << reward << endl;\n",
    "\n",
    "            // Update the true state\n",
    "            currentState = propagationResult->nextState;\n",
    "            currentDiscount *= problemEnvironmentOptions_->discountFactor;\n",
    "\n",
    "            // Check if we're terminal\n",
    "            bool terminal = robotExecutionEnvironment_->isTerminal(propagationResult);\n",
    "\n",
    "            if (terminal) {\n",
    "                PRINT(\"Terminal state reached!\");\n",
    "                finalState = currentState;\n",
    "                if (reward == robotExecutionEnvironment_->getRewardPlugin()->getMinMaxReward().second)\n",
    "                    success = true;\n",
    "            }\n",
    "\n",
    "            // Update the belief\n",
    "            if (!solver_->updateBelief(action, observationResult->observation, terminal)) {\n",
    "                // Couldn't update the belief\n",
    "                finalState = currentState;\n",
    "                serializeLastBelief = false;\n",
    "                LOGGING(\"Couldn't update belief. Ending simulation run\");\n",
    "ï¿¼\n",
    "\n",
    "                break;\n",
    "            }\n",
    "\n",
    "            if (problemEnvironmentOptions_->hasVerboseOutput)\n",
    "                PRINT(\"Updated belief\");\n",
    "\n",
    "            beliefParticles = solver_->getBeliefParticles();\n",
    "\n",
    "            // Update the viewer\n",
    "            if (viewer && viewer->viewerRunning())\n",
    "                updateViewer(currentState, beliefParticles);\n",
    "\n",
    "\n",
    "            // Serialization\n",
    "            ... Some code for serializing steps ...\n",
    "\n",
    "            // Update step\n",
    "            currentStep++;\n",
    "\n",
    "            // Input between steps\n",
    "            **getchar();**\n",
    "\n",
    "            if (terminal)\n",
    "                break;\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbac2ea-d7e2-4f93-85c2-51780fce9078",
   "metadata": {},
   "source": [
    "# Tiger Problem Plugins Implementation\n",
    "**Simple Tiger Problem Example**\n",
    "\n",
    "Ref: https://h2r.github.io/pomdp-py/html/examples.tiger.html#:~:text=The%20description%20of%20the%20tiger,put%20behind%20the%20other%20one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640c0d8-fad4-49fd-ae99-2f5d2f363d14",
   "metadata": {},
   "source": [
    "## Anatomy of Config File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa720eb-7e22-4d48-b9c8-b9d0ab81497b",
   "metadata": {},
   "source": [
    "### General-purpose settings (These keywords are used to control the details on the log files produced)\n",
    "\n",
    "__Verbose output__\n",
    "\n",
    "verbose = true     \n",
    "\n",
    "__Logfile output path__\n",
    "\n",
    "logPath = /home/jimy/Desktop/testing                      \n",
    "\n",
    "__Replace log files with same destination__\n",
    "\n",
    "overwriteExistingLogFiles = false       \n",
    "\n",
    "__keyword postfix to be added to the log file name__\n",
    "logFilePostfix = tiger_problem_test   \n",
    "\n",
    "__Include detailed belief output (individual particles for belief nodes at each step would be logged)__\n",
    "\n",
    "saveParticles = false                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dc6b3-56f9-4149-980b-c3f266d27ad4",
   "metadata": {},
   "source": [
    "### Defining State, Action, Observation Spaces\n",
    "All 3 spaces can be automatically produced from information from the configuration file.\n",
    "However, producing the spaces in this manner has some limitations. \n",
    "\n",
    "This section will present different ways of redefining or setting your POMDP spaces.\n",
    "- Bypass state space\n",
    "- Custom Action Space definition\n",
    "- Observation Space from config file\n",
    "\n",
    "## State space definition\n",
    "\n",
    "- Issue: Currently Error when trying to \"set\" state space to be 1-D through additionalDimensions in config:\n",
    "Workaround is to define custom state space. The states considered for the state space are the states that appear from the\n",
    "        - Initial Belief Plugin (Sampling an initial state)\n",
    "        - Transition Plugin (How the states evolve)\n",
    "    \n",
    "We can encode that coverage of the state space by defining how the initial state is sampled and how states evolve.\n",
    "(Show Initial Belief Plugin Implementation)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1cd22-16cd-4712-bfd5-b610ba50ea5f",
   "metadata": {},
   "source": [
    "## Problem with CFG Definition of Action Space\n",
    "With the custom cfg definition of the action space (e.g)\n",
    "\n",
    "Setting \"additionalDimensions = 6\" with the appropiate limits:\n",
    "- Considering each dimension has 2 steps:\n",
    "\n",
    "We have a total of 64 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083acf35-9ee8-46b7-82eb-7791c2b4364a",
   "metadata": {},
   "source": [
    "## Custom Action Space definition\n",
    "\n",
    "Currently, the action space can be automatically defined in the configuration file.\n",
    "However, the current use of the action space definition relies on the following s inputs\n",
    "- Num of dimensions of space:\n",
    "- actionType = discrete\n",
    "- numInputStepsActions\n",
    "\n",
    "The problem with this definition is that customing an action space (with custom action vector represenations is tricky\n",
    "(E.g Robot Problem had 6 dimensional actions (joint1, joint2, ... joint 6)  Each action moved 6 joints. However, to include additional individual actions (Open Gripper) would have\n",
    "required an additional dimension (which by default could increase the action space complexity unintendedly)\n",
    "\n",
    "\n",
    "**Can Customize Action Space by implementing custom class inheriting from ActionSpaceDiscretizer.hpp**\n",
    "\n",
    "(Look at TigerProblemActionSpaceDiscretizer.hpp for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981c461-15a9-4119-ad02-3b7074742f97",
   "metadata": {},
   "source": [
    "## Observation Space Definition\n",
    "For the tiger problem, we can define the observation space using the __discrete__ type in the config file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
